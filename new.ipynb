{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Early Stopping***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        # number of epochs to wait for improvement before stopping the training.\n",
    "        self.patience = patience\n",
    "        # Minimum change in the monitored quantity validation loss to be considered as an improvement.\n",
    "        self.min_delta = min_delta\n",
    "        #A boolean flag indicating whether to restore the model's weights to the best observed during training when early stopping is triggered.\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        #1- Initialization Check:\n",
    "        if self.best_loss is None:\n",
    "            #it means that this is the first time the method is being called. In this case, it initializes self.best_loss with the current val_loss\n",
    "            self.best_loss = val_loss\n",
    "            #it deep copies the state of the model (model.state_dict()) into self.best_model.\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        #2- Improvement Check:\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            #Indicates an improvement in validation loss greater than or equal to the minimum delta required for considering it as an improvement. \n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            #Resets the counter to 0, and updates the status message.\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        #3- No Improvement:\n",
    "\n",
    "        else:\n",
    "            #ncrements the self.counter to indicate another epoch without improvement.\n",
    "            self.counter += 1\n",
    "            \n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                #If the counter reaches or exceeds the patience threshold (self.patience), it triggers early stopping.\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    #if self.restore_best_weights is True, it loads the best model weights (self.best_model) back to the model.\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True #Indicating that early stopping has been triggered.\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Drop_Out***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dropout():\n",
    "    def __init__(self, p):\n",
    "        self.mask = None\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, X, mode):\n",
    "        return self.forward(X, mode)\n",
    "    \n",
    "    def forward(self, X, mode):\n",
    "        if mode == 'train':\n",
    "            self.mask = np.random.binomial(1, self.p, x.shape)\n",
    "        #np.random.binomial is a NumPy function that generates random numbers from a binomial distribution.\n",
    "        #The first argument 1 specifies that each element in the output will be either 0 or 1 (success or failure).\n",
    "        #The second argument self.p represents the probability of success for each trial (the probability of keeping a neuron active during dropout).\n",
    "        #The third argument x.shape determines the shape of the output array, which is the same shape as the input X.\n",
    "        #This line generates a binary mask where each element has a value of 1 with probability self.p and 0 with probability (1 - self.p).\n",
    "        \n",
    "            self.mask = np.true_divide(self.mask, self.p)\n",
    "        #p.true_divide is a NumPy function that performs element-wise division.\n",
    "        #This line scales the mask by dividing each element by self.p.\n",
    "        #Scaling by self.p ensures that the expected value of the output remains the same, effectively compensating for the dropout effect during the forward pass.\n",
    "            out =  self.mask * X\n",
    "        else:\n",
    "            out = X\n",
    "     \n",
    "        return out\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask\n",
    "        #This ensures that only the active neurons (those that weren't zeroed out during the forward pass) contribute to the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***L1-Regularization (LASSO)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a class for Lasso Regression(L1-Regularization)\n",
    "\n",
    "class Lasso_Regression():\n",
    "\n",
    "  #initiating the hyperparameters\n",
    "  def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n",
    "\n",
    "    self.learning_rate = learning_rate\n",
    "    self.no_of_iterations = no_of_iterations\n",
    "    self.lambda_parameter = lambda_parameter\n",
    "\n",
    "\n",
    "  # fitting the dataset to the Lasso Regression model\n",
    "  def fit(self, X, Y):\n",
    "\n",
    "    # m --> number of Data points --> number of rows\n",
    "    # n --> number of input features --> number of columns\n",
    "    self.m, self.n = X.shape\n",
    "\n",
    "    self.w = np.zeros(self.n)\n",
    "\n",
    "    self.b = 0\n",
    "\n",
    "    self.X = X\n",
    "\n",
    "    self.Y = Y\n",
    "\n",
    "    # implementing Gradient Descent algorithm for Optimization\n",
    "\n",
    "    for i in range(no_of_iterations):\n",
    "      self.upadte_weights()\n",
    "\n",
    "\n",
    "  # function for updating the weight & bias value\n",
    "  def upadte_weights(self):\n",
    "\n",
    "    # linear equation of the model\n",
    "    Y_prediction = self.predict(self.X)\n",
    "\n",
    "    # gradients (dw, db)\n",
    "\n",
    "    # gradient for weight\n",
    "    dw = np.zeros(self.n)\n",
    "\n",
    "    for i in range(self.n):\n",
    "\n",
    "      if self.w[i]>0:\n",
    "\n",
    "        dw[i] = (-(2*(self.X[:,i]).dot(self.Y - Y_prediction)) + self.lambda_parameter) / self.m \n",
    "\n",
    "      else :\n",
    "\n",
    "        dw[i] = (-(2*(self.X[:,i]).dot(self.Y - Y_prediction)) - self.lambda_parameter) / self.m\n",
    "\n",
    "\n",
    "    # gradient for bias\n",
    "    db = - 2 * np.sum(self.Y - Y_prediction) / self.m\n",
    "\n",
    "\n",
    "    # updating the weights & bias\n",
    "\n",
    "    self.w = self.w - self.learning_rate*dw\n",
    "    self.b = self.b - self.learning_rate*db\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  # Predicting the Target variable\n",
    "  def predict(self,X):\n",
    "\n",
    "    return X.dot(self.w) + self.b\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
